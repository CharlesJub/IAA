---
title: "R Notebook"
output: html_notebook
---

## Breakout
```{r}
bike <- read.csv('https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/bike.csv')
```

You want to build a couple of different models and see which one is better. We will learn in later lectures how to do this with test datasets, but for right now we will only do this with training data. First, we need to split the data into training and test. Run the following code to get the training and test split:

```{r}
set.seed(123) 
bike <- bike %>% mutate(id = row_number()) 
train <- bike %>% sample_frac(0.7) 
test <- anti_join(bike, train, by = 'id')
```
What is the observation count in each dataset (train and test)?
train - 12165
test - 5214

You canâ€™t decide which variable is better to predict number of users (cnt), actual temperature (temp) or what the temperature feels like (atemp). You know they are highly correlated (correlation of approximately 0.99), but you feel that each might still provide some valuable information. Maybe the temperature provides (ever so slightly) different information than the feeling temperature. To keep them both in the model, build a ridge regression that has four variables: actual temperature (temp), feeling temperature (atemp), humidity (hum) and wind speed (windspeed). Use CV to find the penalty that minimizes the MSE. What is that lambda value?

7.388165

```{r create dummy train}
train_reg <- train %>% 
  dplyr::select(cnt, temp, atemp, hum, windspeed) %>% 
  mutate_if(is.numeric, ~replace_na(., mean(., na.rm = TRUE))) 

train_x <- model.matrix(cnt ~ ., data = train_reg)[, -1] 
train_y <- train_reg$cnt
```

```{r create dummy test}
test_reg <- test %>% 
  dplyr::select(cnt, temp, atemp, hum, windspeed) %>% 
  mutate_if(is.numeric, ~replace_na(., mean(., na.rm = TRUE))) 

test_x <- model.matrix(cnt ~ ., data = test_reg)[, -1] 
test_y <- test_reg$cnt
```

```{r cv ridge}
library(glmnet) 
bike_ridge <- glmnet(x = train_x, y = train_y, alpha = 0) 


bike_ridge_cv <- cv.glmnet(x = train_x, y = train_y, alpha = 0) 
plot(bike_ridge_cv)
bike_ridge_cv$lambda.min
```
Get predictions for this ridge regression on the test dataset. What are the first 5 predictions from the test dataset from this model? What are the respective actual values of cnt from this dataset?

Predictions:
```{r first 5 predictions}
test$pred <- predict(bike_ridge, s = bike_ridge_cv$lambda.min, newx = test_x)
test$pred %>% head(5)
```
Real:
```{r real}
test$cnt %>% head(5)
```

A MLR with only feeling temperature, humidity, and windspeed had a test dataset MAPE of 478%. What was the MAPE from your ridge regression? From this comparison, which model would you choose?

The MLR has better MAPE so we should use that model.

```{r}
test %>% 
  mutate(ridge_APE = 100*abs((cnt - pred)/cnt)) %>% 
  dplyr::summarise(MAPE_ridge = mean(ridge_APE))
```


## Lab



```{r load data}
library(AppliedPredictiveModeling) 
data(FuelEconomy)
```

```{r factor vars}
cars2010_factor <- cars2010 %>% mutate(across(-c(FE, EngDispl), as.factor))
```

```{r make cars dummy}
set.seed(123)
cars2010_reg <- cars2010_factor %>% 
  mutate_if(is.numeric, ~replace_na(., mean(., na.rm = TRUE))) 

train_x <- model.matrix(FE ~ ., data = cars2010_reg)[, -1] 
train_y <- cars2010_reg$FE
```

```{r LASSO}
cars_lasso <- glmnet(x = train_x, y = train_y, alpha = 1) 
plot(cars_lasso, xvar = "lambda")
```
Perform a CV LASSO to optimize the lambda value. 
a. What is the value of lambda that minimizes the MSE? 
0.003793359
b. What is the value of lambda one standard error above the minimum MSE value?
0.1428175
c. How many variables are left at the penalty value that is one standard error above the minimum MSE value (think of variables as a whole, not per category)? (HINT: Look at the coefficients from the model with coef function.)
11

```{r LASSO CV}
set.seed(123)
cars_lasso_cv <- cv.glmnet(x = train_x, y = train_y, alpha = 1) 
cars_lasso_cv$lambda.min
cars_lasso_cv$lambda.1se
important <- abs(coef(cars_lasso, s = c(cars_lasso_cv$lambda.min, cars_lasso_cv$lambda.1se))[,1]) > 0 &
  abs(coef(cars_lasso, s = c(cars_lasso_cv$lambda.min, cars_lasso_cv$lambda.1se))[,2]) > 0

coef(cars_lasso, s = c(cars_lasso_cv$lambda.min, cars_lasso_cv$lambda.1se))[important,1]

```
Obtain the variables from the LASSO regression at the penalty value that is one standard error above the minimum MSE value. The multiple linear regression with p-value selection (Lab 6) left the variables EngDispl, NumCyl, Transmission, AirAspirationMethod, NumGears, TransLockup, DriveDesc, IntakeValvePerCyl, CarlineClassDesc, and VarValveLift. 
a. What variables were left in your LASSO at the 1SE above minimum MSE penalty value? 
```{r}
coef(cars_lasso, s = cars_lasso_cv$lambda.1se)[(abs(coef(cars_lasso, s = cars_lasso_cv$lambda.1se)[,1]) > 0), 1]
```
b. How does this compare against the variables from the multiple linear regression?











